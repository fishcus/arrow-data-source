{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Arrow Data Source A Spark DataSource implementation for reading files into Arrow compatible columnar vectors. Note The development of this library is still in progress. As a result some of the functionality may not be constantly stable for being used in production environments that have not been fully considered due to the limited testing capabilities so far. Build Prerequisite There are some requirements before you build the project. Please make sure you have already installed the software in your system. gcc 9.3 or higher version java8 OpenJDK -> yum install java-1.8.0-openjdk cmake 3.2 or higher version maven 3.1.1 or higher version Hadoop 2.7.5 or higher version Spark 3.0.0 or higher version Intel Optimized Arrow 0.17.0 Building by Conda If you already have a working Hadoop Spark Cluster, we provide a Conda package which will automatically install dependencies needed by OAP, you can refer to OAP-Installation-Guide for more information. Once finished OAP-Installation-Guide , you can find built spark-arrow-datasource-standard-<version>-jar-with-dependencies.jar under $HOME/miniconda2/envs/oapenv/oap_jars . Then you can just skip steps below and jump to Get Started . cmake installation If you are facing some trouble when installing cmake, please follow below steps to install cmake. // installing cmake 3.2 sudo yum install cmake3 // If you have an existing cmake, you can use below command to set it as an option within alternatives command sudo alternatives --install /usr/local/bin/cmake cmake /usr/bin/cmake 10 --slave /usr/local/bin/ctest ctest /usr/bin/ctest --slave /usr/local/bin/cpack cpack /usr/bin/cpack --slave /usr/local/bin/ccmake ccmake /usr/bin/ccmake --family cmake // Set cmake3 as an option within alternatives command sudo alternatives --install /usr/local/bin/cmake cmake /usr/bin/cmake3 20 --slave /usr/local/bin/ctest ctest /usr/bin/ctest3 --slave /usr/local/bin/cpack cpack /usr/bin/cpack3 --slave /usr/local/bin/ccmake ccmake /usr/bin/ccmake3 --family cmake // Use alternatives to choose cmake version sudo alternatives --config cmake maven installation If you are facing some trouble when installing maven, please follow below steps to install maven // installing maven 3.6.3 Go to https://maven.apache.org/download.cgi and download the specific version of maven // Below command use maven 3.6.3 as an example wget https://ftp.wayne.edu/apache/maven/maven-3/3.6.3/binaries/apache-maven-3.6.3-bin.tar.gz tar xzf apache-maven-3.6.3-bin.tar.gz mkdir /usr/local/maven mv apache-maven-3.6.3/ /usr/local/maven/ // Set maven 3.6.3 as an option within alternatives command sudo alternatives --install /usr/bin/mvn mvn /usr/local/maven/apache-maven-3.6.3/bin/mvn 1 // Use alternatives to choose mvn version sudo alternatives --config mvn Hadoop Native Library(Default) Please make sure you have set up Hadoop directory properly with Hadoop Native Libraries By default, Apache Arrow would scan $HADOOP_HOME and find the native Hadoop library libhdfs.so (under $HADOOP_HOME/lib/native directory) to be used for Hadoop client. You can also use ARROW_LIBHDFS_DIR to configure the location of libhdfs.so if it is installed in other directory than $HADOOP_HOME/lib/native If your SPARK and HADOOP are separated in different nodes, please find libhdfs.so in your Hadoop cluster and copy it to SPARK cluster, then use one of the above methods to set it properly. For more information, please check Arrow HDFS interface documentation Hadoop Native Library, please read the official Hadoop website documentation Use libhdfs3 library for better performance(Optional) For better performance ArrowDataSource reads HDFS files using the third-party library libhdfs3 . The library must be pre-installed on machines Spark Executor nodes are running on. To install the library, use of Conda is recommended. // installing libhdfs3 conda install -c conda-forge libhdfs3 // check the installed library file ll ~/miniconda/envs/$(YOUR_ENV_NAME)/lib/libhdfs3.so To set up libhdfs3, there are two different ways: Option1: Overwrite the soft link for libhdfs.so To install libhdfs3.so, you have to create a soft link for libhdfs.so in your Hadoop directory( $HADOOP_HOME/lib/native by default). ln -f -s libhdfs3.so libhdfs.so Option2: Add env variable to the system export ARROW_LIBHDFS3_DIR=\"PATH_TO_LIBHDFS3_DIR/\" Add following Spark configuration options before running the DataSource to make the library to be recognized: * spark.executorEnv.ARROW_LIBHDFS3_DIR = \"PATH_TO_LIBHDFS3_DIR/\" * spark.executorEnv.LD_LIBRARY_PATH = \"PATH_TO_LIBHDFS3_DEPENDENCIES_DIR/\" Please notes: If you choose to use libhdfs3.so, there are some other dependency libraries you have to installed such as libprotobuf or libcrypto. Build and install Intel\u00ae Optimized Arrow with Datasets Java API You have to use a customized Arrow to support for our datasets Java API. // build arrow-cpp git clone -b <version> https://github.com/Intel-bigdata/arrow.git cd arrow/cpp mkdir build cd build cmake -DARROW_DEPENDENCY_SOURCE=BUNDLED -DARROW_GANDIVA_JAVA=ON -DARROW_GANDIVA=ON -DARROW_PARQUET=ON -DARROW_HDFS=ON -DARROW_BOOST_USE_SHARED=ON -DARROW_JNI=ON -DARROW_DATASET=ON -DARROW_WITH_PROTOBUF=ON -DARROW_WITH_SNAPPY=ON -DARROW_WITH_LZ4=ON -DARROW_FILESYSTEM=ON -DARROW_JSON=ON .. make // build and install arrow jvm library cd ../../java mvn clean install -P arrow-jni -am -Darrow.cpp.build.dir=$PATH_TO_ARROW_SOURCE_CODE/arrow/cpp/build/release Build Arrow Data Source Library // Download Arrow Data Source Code git clone -b <version> https://github.com/oap-project/arrow-data-source.git // Go to the directory cd arrow-data-source // build mvn clean -DskipTests package // check built jar library readlink -f standard/target/spark-arrow-datasource-standard-<version>-jar-with-dependencies.jar Download Spark 3.0.0 Currently ArrowDataSource works on the Spark 3.0.0 version. wget http://archive.apache.org/dist/spark/spark-3.0.0/spark-3.0.0-bin-hadoop2.7.tgz tar -xf ./spark-3.0.0-bin-hadoop2.7.tgz export SPARK_HOME=`pwd`/spark-3.0.0-bin-hadoop2.7 If you are new to Apache Spark, please go though Spark's official deploying guide before getting started with ArrowDataSource. Get started Add extra class pathes to Spark To enable ArrowDataSource, the previous built jar spark-arrow-datasource-standard-<version>-jar-with-dependencies.jar should be added to Spark configuration. Typically the options are: spark.driver.extraClassPath : Set to load jar file to driver. spark.executor.extraClassPath : Set to load jar file to executor. jars : Set to copy jar file to the executors when using yarn cluster mode. spark.executorEnv.ARROW_LIBHDFS3_DIR : Optional if you are using a custom libhdfs3.so. spark.executorEnv.LD_LIBRARY_PATH : Optional if you are using a custom libhdfs3.so. For Spark Standalone Mode, please set the above value as relative path to the jar file. For Spark Yarn Cluster Mode, please set the above value as absolute path to the jar file. Example to run Spark Shell with ArrowDataSource jar file ${SPARK_HOME}/bin/spark-shell \\ --verbose \\ --master yarn \\ --driver-memory 10G \\ --conf spark.driver.extraClassPath=$PATH_TO_DATASOURCE_DIR/spark-arrow-datasource-standard-<version>-jar-with-dependencies.jar \\ --conf spark.executor.extraClassPath=$PATH_TO_DATASOURCE_DIR/spark-arrow-datasource-standard-<version>-jar-with-dependencies.jar \\ --conf spark.driver.cores=1 \\ --conf spark.executor.instances=12 \\ --conf spark.executor.cores=6 \\ --conf spark.executor.memory=20G \\ --conf spark.memory.offHeap.size=80G \\ --conf spark.task.cpus=1 \\ --conf spark.locality.wait=0s \\ --conf spark.sql.shuffle.partitions=72 \\ --conf spark.executorEnv.ARROW_LIBHDFS3_DIR=\"$PATH_TO_LIBHDFS3_DIR/\" \\ --conf spark.executorEnv.LD_LIBRARY_PATH=\"$PATH_TO_LIBHDFS3_DEPENDENCIES_DIR\" For more information about these options, please read the official Spark documentation . Run a query with ArrowDataSource (Scala) val path = \"${PATH_TO_YOUR_PARQUET_FILE}\" val df = spark.read .option(ArrowOptions.KEY_ORIGINAL_FORMAT, \"parquet\") .option(ArrowOptions.KEY_FILESYSTEM, \"hdfs\") .format(\"arrow\") .load(path) df.createOrReplaceTempView(\"my_temp_view\") spark.sql(\"SELECT * FROM my_temp_view LIMIT 10\").show(10) To validate if ArrowDataSource works properly To validate if ArrowDataSource works, you can go to the DAG to check if ArrowScan has been used from the above example query. Work together with ParquetDataSource (experimental) We provide a customized replacement of Spark's built-in ParquetFileFormat. By so users don't have to change existing Parquet-based SQL/code and will be able to read Arrow data from Parquet directly. More importantly, sometimes the feature could be extremely helpful to make ArrowDataSource work correctly with some 3rd-party storage tools (e.g. Delta Lake ) that are built on top of ParquetDataSource. To replace built-in ParquetDataSource, the only thing has to be done is to place compiled jar spark-arrow-datasource-parquet-<version>.jar into Spark's library folder. If you'd like to verify that ParquetDataSource is successfully overwritten by the jar, run following code before executing SQL job: ServiceLoaderUtil.ensureParquetFileFormatOverwritten(); Note the whole feature is currently experimental and only DataSource v1 is supported. V2 support is being planned.","title":"Arrow Data Source"},{"location":"#arrow-data-source","text":"A Spark DataSource implementation for reading files into Arrow compatible columnar vectors.","title":"Arrow Data Source"},{"location":"#note","text":"The development of this library is still in progress. As a result some of the functionality may not be constantly stable for being used in production environments that have not been fully considered due to the limited testing capabilities so far.","title":"Note"},{"location":"#build","text":"","title":"Build"},{"location":"#prerequisite","text":"There are some requirements before you build the project. Please make sure you have already installed the software in your system. gcc 9.3 or higher version java8 OpenJDK -> yum install java-1.8.0-openjdk cmake 3.2 or higher version maven 3.1.1 or higher version Hadoop 2.7.5 or higher version Spark 3.0.0 or higher version Intel Optimized Arrow 0.17.0","title":"Prerequisite"},{"location":"#building-by-conda","text":"If you already have a working Hadoop Spark Cluster, we provide a Conda package which will automatically install dependencies needed by OAP, you can refer to OAP-Installation-Guide for more information. Once finished OAP-Installation-Guide , you can find built spark-arrow-datasource-standard-<version>-jar-with-dependencies.jar under $HOME/miniconda2/envs/oapenv/oap_jars . Then you can just skip steps below and jump to Get Started .","title":"Building by Conda"},{"location":"#cmake-installation","text":"If you are facing some trouble when installing cmake, please follow below steps to install cmake. // installing cmake 3.2 sudo yum install cmake3 // If you have an existing cmake, you can use below command to set it as an option within alternatives command sudo alternatives --install /usr/local/bin/cmake cmake /usr/bin/cmake 10 --slave /usr/local/bin/ctest ctest /usr/bin/ctest --slave /usr/local/bin/cpack cpack /usr/bin/cpack --slave /usr/local/bin/ccmake ccmake /usr/bin/ccmake --family cmake // Set cmake3 as an option within alternatives command sudo alternatives --install /usr/local/bin/cmake cmake /usr/bin/cmake3 20 --slave /usr/local/bin/ctest ctest /usr/bin/ctest3 --slave /usr/local/bin/cpack cpack /usr/bin/cpack3 --slave /usr/local/bin/ccmake ccmake /usr/bin/ccmake3 --family cmake // Use alternatives to choose cmake version sudo alternatives --config cmake","title":"cmake installation"},{"location":"#maven-installation","text":"If you are facing some trouble when installing maven, please follow below steps to install maven // installing maven 3.6.3 Go to https://maven.apache.org/download.cgi and download the specific version of maven // Below command use maven 3.6.3 as an example wget https://ftp.wayne.edu/apache/maven/maven-3/3.6.3/binaries/apache-maven-3.6.3-bin.tar.gz tar xzf apache-maven-3.6.3-bin.tar.gz mkdir /usr/local/maven mv apache-maven-3.6.3/ /usr/local/maven/ // Set maven 3.6.3 as an option within alternatives command sudo alternatives --install /usr/bin/mvn mvn /usr/local/maven/apache-maven-3.6.3/bin/mvn 1 // Use alternatives to choose mvn version sudo alternatives --config mvn","title":"maven installation"},{"location":"#hadoop-native-librarydefault","text":"Please make sure you have set up Hadoop directory properly with Hadoop Native Libraries By default, Apache Arrow would scan $HADOOP_HOME and find the native Hadoop library libhdfs.so (under $HADOOP_HOME/lib/native directory) to be used for Hadoop client. You can also use ARROW_LIBHDFS_DIR to configure the location of libhdfs.so if it is installed in other directory than $HADOOP_HOME/lib/native If your SPARK and HADOOP are separated in different nodes, please find libhdfs.so in your Hadoop cluster and copy it to SPARK cluster, then use one of the above methods to set it properly. For more information, please check Arrow HDFS interface documentation Hadoop Native Library, please read the official Hadoop website documentation","title":"Hadoop Native Library(Default)"},{"location":"#use-libhdfs3-library-for-better-performanceoptional","text":"For better performance ArrowDataSource reads HDFS files using the third-party library libhdfs3 . The library must be pre-installed on machines Spark Executor nodes are running on. To install the library, use of Conda is recommended. // installing libhdfs3 conda install -c conda-forge libhdfs3 // check the installed library file ll ~/miniconda/envs/$(YOUR_ENV_NAME)/lib/libhdfs3.so To set up libhdfs3, there are two different ways: Option1: Overwrite the soft link for libhdfs.so To install libhdfs3.so, you have to create a soft link for libhdfs.so in your Hadoop directory( $HADOOP_HOME/lib/native by default). ln -f -s libhdfs3.so libhdfs.so Option2: Add env variable to the system export ARROW_LIBHDFS3_DIR=\"PATH_TO_LIBHDFS3_DIR/\" Add following Spark configuration options before running the DataSource to make the library to be recognized: * spark.executorEnv.ARROW_LIBHDFS3_DIR = \"PATH_TO_LIBHDFS3_DIR/\" * spark.executorEnv.LD_LIBRARY_PATH = \"PATH_TO_LIBHDFS3_DEPENDENCIES_DIR/\" Please notes: If you choose to use libhdfs3.so, there are some other dependency libraries you have to installed such as libprotobuf or libcrypto.","title":"Use libhdfs3 library for better performance(Optional)"},{"location":"#build-and-install-intel-optimized-arrow-with-datasets-java-api","text":"You have to use a customized Arrow to support for our datasets Java API. // build arrow-cpp git clone -b <version> https://github.com/Intel-bigdata/arrow.git cd arrow/cpp mkdir build cd build cmake -DARROW_DEPENDENCY_SOURCE=BUNDLED -DARROW_GANDIVA_JAVA=ON -DARROW_GANDIVA=ON -DARROW_PARQUET=ON -DARROW_HDFS=ON -DARROW_BOOST_USE_SHARED=ON -DARROW_JNI=ON -DARROW_DATASET=ON -DARROW_WITH_PROTOBUF=ON -DARROW_WITH_SNAPPY=ON -DARROW_WITH_LZ4=ON -DARROW_FILESYSTEM=ON -DARROW_JSON=ON .. make // build and install arrow jvm library cd ../../java mvn clean install -P arrow-jni -am -Darrow.cpp.build.dir=$PATH_TO_ARROW_SOURCE_CODE/arrow/cpp/build/release","title":"Build and install Intel\u00ae Optimized Arrow with Datasets Java API"},{"location":"#build-arrow-data-source-library","text":"// Download Arrow Data Source Code git clone -b <version> https://github.com/oap-project/arrow-data-source.git // Go to the directory cd arrow-data-source // build mvn clean -DskipTests package // check built jar library readlink -f standard/target/spark-arrow-datasource-standard-<version>-jar-with-dependencies.jar","title":"Build Arrow Data Source Library"},{"location":"#download-spark-300","text":"Currently ArrowDataSource works on the Spark 3.0.0 version. wget http://archive.apache.org/dist/spark/spark-3.0.0/spark-3.0.0-bin-hadoop2.7.tgz tar -xf ./spark-3.0.0-bin-hadoop2.7.tgz export SPARK_HOME=`pwd`/spark-3.0.0-bin-hadoop2.7 If you are new to Apache Spark, please go though Spark's official deploying guide before getting started with ArrowDataSource.","title":"Download Spark 3.0.0"},{"location":"#get-started","text":"","title":"Get started"},{"location":"#add-extra-class-pathes-to-spark","text":"To enable ArrowDataSource, the previous built jar spark-arrow-datasource-standard-<version>-jar-with-dependencies.jar should be added to Spark configuration. Typically the options are: spark.driver.extraClassPath : Set to load jar file to driver. spark.executor.extraClassPath : Set to load jar file to executor. jars : Set to copy jar file to the executors when using yarn cluster mode. spark.executorEnv.ARROW_LIBHDFS3_DIR : Optional if you are using a custom libhdfs3.so. spark.executorEnv.LD_LIBRARY_PATH : Optional if you are using a custom libhdfs3.so. For Spark Standalone Mode, please set the above value as relative path to the jar file. For Spark Yarn Cluster Mode, please set the above value as absolute path to the jar file. Example to run Spark Shell with ArrowDataSource jar file ${SPARK_HOME}/bin/spark-shell \\ --verbose \\ --master yarn \\ --driver-memory 10G \\ --conf spark.driver.extraClassPath=$PATH_TO_DATASOURCE_DIR/spark-arrow-datasource-standard-<version>-jar-with-dependencies.jar \\ --conf spark.executor.extraClassPath=$PATH_TO_DATASOURCE_DIR/spark-arrow-datasource-standard-<version>-jar-with-dependencies.jar \\ --conf spark.driver.cores=1 \\ --conf spark.executor.instances=12 \\ --conf spark.executor.cores=6 \\ --conf spark.executor.memory=20G \\ --conf spark.memory.offHeap.size=80G \\ --conf spark.task.cpus=1 \\ --conf spark.locality.wait=0s \\ --conf spark.sql.shuffle.partitions=72 \\ --conf spark.executorEnv.ARROW_LIBHDFS3_DIR=\"$PATH_TO_LIBHDFS3_DIR/\" \\ --conf spark.executorEnv.LD_LIBRARY_PATH=\"$PATH_TO_LIBHDFS3_DEPENDENCIES_DIR\" For more information about these options, please read the official Spark documentation .","title":"Add extra class pathes to Spark"},{"location":"#run-a-query-with-arrowdatasource-scala","text":"val path = \"${PATH_TO_YOUR_PARQUET_FILE}\" val df = spark.read .option(ArrowOptions.KEY_ORIGINAL_FORMAT, \"parquet\") .option(ArrowOptions.KEY_FILESYSTEM, \"hdfs\") .format(\"arrow\") .load(path) df.createOrReplaceTempView(\"my_temp_view\") spark.sql(\"SELECT * FROM my_temp_view LIMIT 10\").show(10)","title":"Run a query with ArrowDataSource (Scala)"},{"location":"#to-validate-if-arrowdatasource-works-properly","text":"To validate if ArrowDataSource works, you can go to the DAG to check if ArrowScan has been used from the above example query.","title":"To validate if ArrowDataSource works properly"},{"location":"#work-together-with-parquetdatasource-experimental","text":"We provide a customized replacement of Spark's built-in ParquetFileFormat. By so users don't have to change existing Parquet-based SQL/code and will be able to read Arrow data from Parquet directly. More importantly, sometimes the feature could be extremely helpful to make ArrowDataSource work correctly with some 3rd-party storage tools (e.g. Delta Lake ) that are built on top of ParquetDataSource. To replace built-in ParquetDataSource, the only thing has to be done is to place compiled jar spark-arrow-datasource-parquet-<version>.jar into Spark's library folder. If you'd like to verify that ParquetDataSource is successfully overwritten by the jar, run following code before executing SQL job: ServiceLoaderUtil.ensureParquetFileFormatOverwritten(); Note the whole feature is currently experimental and only DataSource v1 is supported. V2 support is being planned.","title":"Work together with ParquetDataSource (experimental)"},{"location":"OAP-Developer-Guide/","text":"OAP Developer Guide This document contains the instructions & scripts on installing necessary dependencies and building OAP. You can get more detailed information from OAP each module below. SQL Index and Data Source Cache PMem Common PMem Shuffle Remote Shuffle OAP MLlib Arrow Data Source Native SQL Engine Building OAP Prerequisites for Building OAP is built with Apache Maven and Oracle Java 8, and mainly required tools to install on your cluster are listed below. Cmake GCC > 7 Memkind Vmemcache HPNL PMDK OneAPI Arrow Requirements for Shuffle Remote PMem Extension If enable Shuffle Remote PMem extension with RDMA, you can refer to PMem Shuffle to configure and validate RDMA in advance. We provide scripts below to help automatically install dependencies above except RDMA , need change to root account, run: # git clone -b <tag-version> https://github.com/Intel-bigdata/OAP.git # cd OAP # sh $OAP_HOME/dev/install-compile-time-dependencies.sh Run the following command to learn more. # sh $OAP_HOME/dev/scripts/prepare_oap_env.sh --help Run the following command to automatically install specific dependency such as Maven. # sh $OAP_HOME/dev/scripts/prepare_oap_env.sh --prepare_maven Building To build OAP package, run command below then you can find a tarball named oap-$VERSION-bin-spark-$VERSION.tar.gz under directory $OAP_HOME/dev/release-package . $ sh $OAP_HOME/dev/compile-oap.sh Building Specified OAP Module, such as oap-cache , run: $ sh $OAP_HOME/dev/compile-oap.sh --oap-cache Running OAP Unit Tests Setup building environment manually for intel MLlib, and if your default GCC version is before 7.0 also need export CC & CXX before using mvn , run $ export CXX=$OAP_HOME/dev/thirdparty/gcc7/bin/g++ $ export CC=$OAP_HOME/dev/thirdparty/gcc7/bin/gcc $ export ONEAPI_ROOT=/opt/intel/inteloneapi $ source /opt/intel/inteloneapi/daal/2021.1-beta07/env/vars.sh $ source /opt/intel/inteloneapi/tbb/2021.1-beta07/env/vars.sh $ source /tmp/oneCCL/build/_install/env/setvars.sh Run all the tests: $ mvn clean test Run Specified OAP Module Unit Test, such as oap-cache : $ mvn clean -pl com.intel.oap:oap-cache -am test Building SQL Index and Data Source Cache with PMem Prerequisites for building with PMem support When using SQL Index and Data Source Cache with PMem, finish steps of Prerequisites for building to ensure needed dependencies have been installed. Building package You can build OAP with PMem support with command below: $ sh $OAP_HOME/dev/compile-oap.sh Or run: $ mvn clean -q -Ppersistent-memory -Pvmemcache -DskipTests package","title":"OAP Developer Guide"},{"location":"OAP-Developer-Guide/#oap-developer-guide","text":"This document contains the instructions & scripts on installing necessary dependencies and building OAP. You can get more detailed information from OAP each module below. SQL Index and Data Source Cache PMem Common PMem Shuffle Remote Shuffle OAP MLlib Arrow Data Source Native SQL Engine","title":"OAP Developer Guide"},{"location":"OAP-Developer-Guide/#building-oap","text":"","title":"Building OAP"},{"location":"OAP-Developer-Guide/#prerequisites-for-building","text":"OAP is built with Apache Maven and Oracle Java 8, and mainly required tools to install on your cluster are listed below. Cmake GCC > 7 Memkind Vmemcache HPNL PMDK OneAPI Arrow Requirements for Shuffle Remote PMem Extension If enable Shuffle Remote PMem extension with RDMA, you can refer to PMem Shuffle to configure and validate RDMA in advance. We provide scripts below to help automatically install dependencies above except RDMA , need change to root account, run: # git clone -b <tag-version> https://github.com/Intel-bigdata/OAP.git # cd OAP # sh $OAP_HOME/dev/install-compile-time-dependencies.sh Run the following command to learn more. # sh $OAP_HOME/dev/scripts/prepare_oap_env.sh --help Run the following command to automatically install specific dependency such as Maven. # sh $OAP_HOME/dev/scripts/prepare_oap_env.sh --prepare_maven","title":"Prerequisites for Building"},{"location":"OAP-Developer-Guide/#building","text":"To build OAP package, run command below then you can find a tarball named oap-$VERSION-bin-spark-$VERSION.tar.gz under directory $OAP_HOME/dev/release-package . $ sh $OAP_HOME/dev/compile-oap.sh Building Specified OAP Module, such as oap-cache , run: $ sh $OAP_HOME/dev/compile-oap.sh --oap-cache","title":"Building"},{"location":"OAP-Developer-Guide/#running-oap-unit-tests","text":"Setup building environment manually for intel MLlib, and if your default GCC version is before 7.0 also need export CC & CXX before using mvn , run $ export CXX=$OAP_HOME/dev/thirdparty/gcc7/bin/g++ $ export CC=$OAP_HOME/dev/thirdparty/gcc7/bin/gcc $ export ONEAPI_ROOT=/opt/intel/inteloneapi $ source /opt/intel/inteloneapi/daal/2021.1-beta07/env/vars.sh $ source /opt/intel/inteloneapi/tbb/2021.1-beta07/env/vars.sh $ source /tmp/oneCCL/build/_install/env/setvars.sh Run all the tests: $ mvn clean test Run Specified OAP Module Unit Test, such as oap-cache : $ mvn clean -pl com.intel.oap:oap-cache -am test","title":"Running OAP Unit Tests"},{"location":"OAP-Developer-Guide/#building-sql-index-and-data-source-cache-with-pmem","text":"","title":"Building SQL Index and Data Source Cache with PMem"},{"location":"OAP-Developer-Guide/#prerequisites-for-building-with-pmem-support","text":"When using SQL Index and Data Source Cache with PMem, finish steps of Prerequisites for building to ensure needed dependencies have been installed.","title":"Prerequisites for building with PMem support"},{"location":"OAP-Developer-Guide/#building-package","text":"You can build OAP with PMem support with command below: $ sh $OAP_HOME/dev/compile-oap.sh Or run: $ mvn clean -q -Ppersistent-memory -Pvmemcache -DskipTests package","title":"Building package"},{"location":"OAP-Installation-Guide/","text":"OAP Installation Guide This document introduces how to install OAP and its dependencies on your cluster nodes by Conda . Follow steps below on every node of your cluster to set right environment for each machine. Contents Prerequisites Installing OAP Configuration Prerequisites OS Requirements We have tested OAP on Fedora 29 and CentOS 7.6 (kernel-4.18.16). We recommend you use Fedora 29 CentOS 7.6 or above . Besides, for Memkind we recommend you use kernel above 3.10 . Conda Requirements Install Conda on your cluster nodes with below commands and follow the prompts on the installer screens.: $ wget -c https://repo.continuum.io/miniconda/Miniconda2-latest-Linux-x86_64.sh $ chmod +x Miniconda2-latest-Linux-x86_64.sh $ bash Miniconda2-latest-Linux-x86_64.sh For changes to take effect, close and re-open your current shell. To test your installation, run the command conda list in your terminal window. A list of installed packages appears if it has been installed correctly. Installing OAP Dependencies below are required by OAP and all of them are included in OAP Conda package, they will be automatically installed in your cluster when you Conda install OAP. Ensure you have activated environment which you created in the previous steps. Arrow Plasma Memkind Vmemcache HPNL PMDK OneAPI Create a conda environment and install OAP Conda package. $ conda create -n oapenv -y python=3.7 $ conda activate oapenv $ conda install -c conda-forge -c intel -y oap=1.0.0 Once finished steps above, you have completed OAP dependencies installation and OAP building, and will find built OAP jars under $HOME/miniconda2/envs/oapenv/oap_jars Extra Steps for Shuffle Remote PMem Extension If you use one of OAP features -- PMmem Shuffle with RDMA , you need to configure and validate RDMA, please refer to PMem Shuffle for the details. Configuration Once finished steps above, make sure libraries installed by Conda can be linked by Spark, please add the following configuration settings to $SPARK_HOME/conf/spark-defaults.conf . spark.executorEnv.LD_LIBRARY_PATH $HOME/miniconda2/envs/oapenv/lib spark.executor.extraLibraryPath $HOME/miniconda2/envs/oapenv/lib spark.driver.extraLibraryPath $HOME/miniconda2/envs/oapenv/lib spark.executor.extraClassPath $HOME/miniconda2/envs/oapenv/oap_jars/$OAP_FEATURE.jar spark.driver.extraClassPath $HOME/miniconda2/envs/oapenv/oap_jars/$OAP_FEATURE.jar And then you can follow the corresponding feature documents for more details to use them.","title":"OAP Installation Guide"},{"location":"OAP-Installation-Guide/#oap-installation-guide","text":"This document introduces how to install OAP and its dependencies on your cluster nodes by Conda . Follow steps below on every node of your cluster to set right environment for each machine.","title":"OAP Installation Guide"},{"location":"OAP-Installation-Guide/#contents","text":"Prerequisites Installing OAP Configuration","title":"Contents"},{"location":"OAP-Installation-Guide/#prerequisites","text":"OS Requirements We have tested OAP on Fedora 29 and CentOS 7.6 (kernel-4.18.16). We recommend you use Fedora 29 CentOS 7.6 or above . Besides, for Memkind we recommend you use kernel above 3.10 . Conda Requirements Install Conda on your cluster nodes with below commands and follow the prompts on the installer screens.: $ wget -c https://repo.continuum.io/miniconda/Miniconda2-latest-Linux-x86_64.sh $ chmod +x Miniconda2-latest-Linux-x86_64.sh $ bash Miniconda2-latest-Linux-x86_64.sh For changes to take effect, close and re-open your current shell. To test your installation, run the command conda list in your terminal window. A list of installed packages appears if it has been installed correctly.","title":"Prerequisites"},{"location":"OAP-Installation-Guide/#installing-oap","text":"Dependencies below are required by OAP and all of them are included in OAP Conda package, they will be automatically installed in your cluster when you Conda install OAP. Ensure you have activated environment which you created in the previous steps. Arrow Plasma Memkind Vmemcache HPNL PMDK OneAPI Create a conda environment and install OAP Conda package. $ conda create -n oapenv -y python=3.7 $ conda activate oapenv $ conda install -c conda-forge -c intel -y oap=1.0.0 Once finished steps above, you have completed OAP dependencies installation and OAP building, and will find built OAP jars under $HOME/miniconda2/envs/oapenv/oap_jars","title":"Installing OAP"},{"location":"OAP-Installation-Guide/#extra-steps-for-shuffle-remote-pmem-extension","text":"If you use one of OAP features -- PMmem Shuffle with RDMA , you need to configure and validate RDMA, please refer to PMem Shuffle for the details.","title":"Extra Steps for Shuffle Remote PMem Extension"},{"location":"OAP-Installation-Guide/#configuration","text":"Once finished steps above, make sure libraries installed by Conda can be linked by Spark, please add the following configuration settings to $SPARK_HOME/conf/spark-defaults.conf . spark.executorEnv.LD_LIBRARY_PATH $HOME/miniconda2/envs/oapenv/lib spark.executor.extraLibraryPath $HOME/miniconda2/envs/oapenv/lib spark.driver.extraLibraryPath $HOME/miniconda2/envs/oapenv/lib spark.executor.extraClassPath $HOME/miniconda2/envs/oapenv/oap_jars/$OAP_FEATURE.jar spark.driver.extraClassPath $HOME/miniconda2/envs/oapenv/oap_jars/$OAP_FEATURE.jar And then you can follow the corresponding feature documents for more details to use them.","title":"Configuration"},{"location":"User-Guide/","text":"Arrow Data Source A Spark DataSource implementation for reading files into Arrow compatible columnar vectors. Note The development of this library is still in progress. As a result some of the functionality may not be constantly stable for being used in production environments that have not been fully considered due to the limited testing capabilities so far. Build Prerequisite There are some requirements before you build the project. Please make sure you have already installed the software in your system. gcc 9.3 or higher version java8 OpenJDK -> yum install java-1.8.0-openjdk cmake 3.2 or higher version maven 3.1.1 or higher version Hadoop 2.7.5 or higher version Spark 3.0.0 or higher version Intel Optimized Arrow 0.17.0 Building by Conda If you already have a working Hadoop Spark Cluster, we provide a Conda package which will automatically install dependencies needed by OAP, you can refer to OAP-Installation-Guide for more information. Once finished OAP-Installation-Guide , you can find built spark-arrow-datasource-standard-<version>-jar-with-dependencies.jar under $HOME/miniconda2/envs/oapenv/oap_jars . Then you can just skip steps below and jump to Get Started . cmake installation If you are facing some trouble when installing cmake, please follow below steps to install cmake. // installing cmake 3.2 sudo yum install cmake3 // If you have an existing cmake, you can use below command to set it as an option within alternatives command sudo alternatives --install /usr/local/bin/cmake cmake /usr/bin/cmake 10 --slave /usr/local/bin/ctest ctest /usr/bin/ctest --slave /usr/local/bin/cpack cpack /usr/bin/cpack --slave /usr/local/bin/ccmake ccmake /usr/bin/ccmake --family cmake // Set cmake3 as an option within alternatives command sudo alternatives --install /usr/local/bin/cmake cmake /usr/bin/cmake3 20 --slave /usr/local/bin/ctest ctest /usr/bin/ctest3 --slave /usr/local/bin/cpack cpack /usr/bin/cpack3 --slave /usr/local/bin/ccmake ccmake /usr/bin/ccmake3 --family cmake // Use alternatives to choose cmake version sudo alternatives --config cmake maven installation If you are facing some trouble when installing maven, please follow below steps to install maven // installing maven 3.6.3 Go to https://maven.apache.org/download.cgi and download the specific version of maven // Below command use maven 3.6.3 as an example wget https://ftp.wayne.edu/apache/maven/maven-3/3.6.3/binaries/apache-maven-3.6.3-bin.tar.gz tar xzf apache-maven-3.6.3-bin.tar.gz mkdir /usr/local/maven mv apache-maven-3.6.3/ /usr/local/maven/ // Set maven 3.6.3 as an option within alternatives command sudo alternatives --install /usr/bin/mvn mvn /usr/local/maven/apache-maven-3.6.3/bin/mvn 1 // Use alternatives to choose mvn version sudo alternatives --config mvn Hadoop Native Library(Default) Please make sure you have set up Hadoop directory properly with Hadoop Native Libraries By default, Apache Arrow would scan $HADOOP_HOME and find the native Hadoop library libhdfs.so (under $HADOOP_HOME/lib/native directory) to be used for Hadoop client. You can also use ARROW_LIBHDFS_DIR to configure the location of libhdfs.so if it is installed in other directory than $HADOOP_HOME/lib/native If your SPARK and HADOOP are separated in different nodes, please find libhdfs.so in your Hadoop cluster and copy it to SPARK cluster, then use one of the above methods to set it properly. For more information, please check Arrow HDFS interface documentation Hadoop Native Library, please read the official Hadoop website documentation Use libhdfs3 library for better performance(Optional) For better performance ArrowDataSource reads HDFS files using the third-party library libhdfs3 . The library must be pre-installed on machines Spark Executor nodes are running on. To install the library, use of Conda is recommended. // installing libhdfs3 conda install -c conda-forge libhdfs3 // check the installed library file ll ~/miniconda/envs/$(YOUR_ENV_NAME)/lib/libhdfs3.so To set up libhdfs3, there are two different ways: Option1: Overwrite the soft link for libhdfs.so To install libhdfs3.so, you have to create a soft link for libhdfs.so in your Hadoop directory( $HADOOP_HOME/lib/native by default). ln -f -s libhdfs3.so libhdfs.so Option2: Add env variable to the system export ARROW_LIBHDFS3_DIR=\"PATH_TO_LIBHDFS3_DIR/\" Add following Spark configuration options before running the DataSource to make the library to be recognized: * spark.executorEnv.ARROW_LIBHDFS3_DIR = \"PATH_TO_LIBHDFS3_DIR/\" * spark.executorEnv.LD_LIBRARY_PATH = \"PATH_TO_LIBHDFS3_DEPENDENCIES_DIR/\" Please notes: If you choose to use libhdfs3.so, there are some other dependency libraries you have to installed such as libprotobuf or libcrypto. Build and install Intel\u00ae Optimized Arrow with Datasets Java API You have to use a customized Arrow to support for our datasets Java API. // build arrow-cpp git clone -b <version> https://github.com/Intel-bigdata/arrow.git cd arrow/cpp mkdir build cd build cmake -DARROW_DEPENDENCY_SOURCE=BUNDLED -DARROW_GANDIVA_JAVA=ON -DARROW_GANDIVA=ON -DARROW_PARQUET=ON -DARROW_HDFS=ON -DARROW_BOOST_USE_SHARED=ON -DARROW_JNI=ON -DARROW_DATASET=ON -DARROW_WITH_PROTOBUF=ON -DARROW_WITH_SNAPPY=ON -DARROW_WITH_LZ4=ON -DARROW_FILESYSTEM=ON -DARROW_JSON=ON .. make // build and install arrow jvm library cd ../../java mvn clean install -P arrow-jni -am -Darrow.cpp.build.dir=$PATH_TO_ARROW_SOURCE_CODE/arrow/cpp/build/release Build Arrow Data Source Library // Download Arrow Data Source Code git clone -b <version> https://github.com/oap-project/arrow-data-source.git // Go to the directory cd arrow-data-source // build mvn clean -DskipTests package // check built jar library readlink -f standard/target/spark-arrow-datasource-standard-<version>-jar-with-dependencies.jar Download Spark 3.0.0 Currently ArrowDataSource works on the Spark 3.0.0 version. wget http://archive.apache.org/dist/spark/spark-3.0.0/spark-3.0.0-bin-hadoop2.7.tgz tar -xf ./spark-3.0.0-bin-hadoop2.7.tgz export SPARK_HOME=`pwd`/spark-3.0.0-bin-hadoop2.7 If you are new to Apache Spark, please go though Spark's official deploying guide before getting started with ArrowDataSource. Get started Add extra class pathes to Spark To enable ArrowDataSource, the previous built jar spark-arrow-datasource-standard-<version>-jar-with-dependencies.jar should be added to Spark configuration. Typically the options are: spark.driver.extraClassPath : Set to load jar file to driver. spark.executor.extraClassPath : Set to load jar file to executor. jars : Set to copy jar file to the executors when using yarn cluster mode. spark.executorEnv.ARROW_LIBHDFS3_DIR : Optional if you are using a custom libhdfs3.so. spark.executorEnv.LD_LIBRARY_PATH : Optional if you are using a custom libhdfs3.so. For Spark Standalone Mode, please set the above value as relative path to the jar file. For Spark Yarn Cluster Mode, please set the above value as absolute path to the jar file. Example to run Spark Shell with ArrowDataSource jar file ${SPARK_HOME}/bin/spark-shell \\ --verbose \\ --master yarn \\ --driver-memory 10G \\ --conf spark.driver.extraClassPath=$PATH_TO_DATASOURCE_DIR/spark-arrow-datasource-standard-<version>-jar-with-dependencies.jar \\ --conf spark.executor.extraClassPath=$PATH_TO_DATASOURCE_DIR/spark-arrow-datasource-standard-<version>-jar-with-dependencies.jar \\ --conf spark.driver.cores=1 \\ --conf spark.executor.instances=12 \\ --conf spark.executor.cores=6 \\ --conf spark.executor.memory=20G \\ --conf spark.memory.offHeap.size=80G \\ --conf spark.task.cpus=1 \\ --conf spark.locality.wait=0s \\ --conf spark.sql.shuffle.partitions=72 \\ --conf spark.executorEnv.ARROW_LIBHDFS3_DIR=\"$PATH_TO_LIBHDFS3_DIR/\" \\ --conf spark.executorEnv.LD_LIBRARY_PATH=\"$PATH_TO_LIBHDFS3_DEPENDENCIES_DIR\" For more information about these options, please read the official Spark documentation . Run a query with ArrowDataSource (Scala) val path = \"${PATH_TO_YOUR_PARQUET_FILE}\" val df = spark.read .option(ArrowOptions.KEY_ORIGINAL_FORMAT, \"parquet\") .option(ArrowOptions.KEY_FILESYSTEM, \"hdfs\") .format(\"arrow\") .load(path) df.createOrReplaceTempView(\"my_temp_view\") spark.sql(\"SELECT * FROM my_temp_view LIMIT 10\").show(10) To validate if ArrowDataSource works properly To validate if ArrowDataSource works, you can go to the DAG to check if ArrowScan has been used from the above example query. Work together with ParquetDataSource (experimental) We provide a customized replacement of Spark's built-in ParquetFileFormat. By so users don't have to change existing Parquet-based SQL/code and will be able to read Arrow data from Parquet directly. More importantly, sometimes the feature could be extremely helpful to make ArrowDataSource work correctly with some 3rd-party storage tools (e.g. Delta Lake ) that are built on top of ParquetDataSource. To replace built-in ParquetDataSource, the only thing has to be done is to place compiled jar spark-arrow-datasource-parquet-<version>.jar into Spark's library folder. If you'd like to verify that ParquetDataSource is successfully overwritten by the jar, run following code before executing SQL job: ServiceLoaderUtil.ensureParquetFileFormatOverwritten(); Note the whole feature is currently experimental and only DataSource v1 is supported. V2 support is being planned.","title":"User Guide"},{"location":"User-Guide/#arrow-data-source","text":"A Spark DataSource implementation for reading files into Arrow compatible columnar vectors.","title":"Arrow Data Source"},{"location":"User-Guide/#note","text":"The development of this library is still in progress. As a result some of the functionality may not be constantly stable for being used in production environments that have not been fully considered due to the limited testing capabilities so far.","title":"Note"},{"location":"User-Guide/#build","text":"","title":"Build"},{"location":"User-Guide/#prerequisite","text":"There are some requirements before you build the project. Please make sure you have already installed the software in your system. gcc 9.3 or higher version java8 OpenJDK -> yum install java-1.8.0-openjdk cmake 3.2 or higher version maven 3.1.1 or higher version Hadoop 2.7.5 or higher version Spark 3.0.0 or higher version Intel Optimized Arrow 0.17.0","title":"Prerequisite"},{"location":"User-Guide/#building-by-conda","text":"If you already have a working Hadoop Spark Cluster, we provide a Conda package which will automatically install dependencies needed by OAP, you can refer to OAP-Installation-Guide for more information. Once finished OAP-Installation-Guide , you can find built spark-arrow-datasource-standard-<version>-jar-with-dependencies.jar under $HOME/miniconda2/envs/oapenv/oap_jars . Then you can just skip steps below and jump to Get Started .","title":"Building by Conda"},{"location":"User-Guide/#cmake-installation","text":"If you are facing some trouble when installing cmake, please follow below steps to install cmake. // installing cmake 3.2 sudo yum install cmake3 // If you have an existing cmake, you can use below command to set it as an option within alternatives command sudo alternatives --install /usr/local/bin/cmake cmake /usr/bin/cmake 10 --slave /usr/local/bin/ctest ctest /usr/bin/ctest --slave /usr/local/bin/cpack cpack /usr/bin/cpack --slave /usr/local/bin/ccmake ccmake /usr/bin/ccmake --family cmake // Set cmake3 as an option within alternatives command sudo alternatives --install /usr/local/bin/cmake cmake /usr/bin/cmake3 20 --slave /usr/local/bin/ctest ctest /usr/bin/ctest3 --slave /usr/local/bin/cpack cpack /usr/bin/cpack3 --slave /usr/local/bin/ccmake ccmake /usr/bin/ccmake3 --family cmake // Use alternatives to choose cmake version sudo alternatives --config cmake","title":"cmake installation"},{"location":"User-Guide/#maven-installation","text":"If you are facing some trouble when installing maven, please follow below steps to install maven // installing maven 3.6.3 Go to https://maven.apache.org/download.cgi and download the specific version of maven // Below command use maven 3.6.3 as an example wget https://ftp.wayne.edu/apache/maven/maven-3/3.6.3/binaries/apache-maven-3.6.3-bin.tar.gz tar xzf apache-maven-3.6.3-bin.tar.gz mkdir /usr/local/maven mv apache-maven-3.6.3/ /usr/local/maven/ // Set maven 3.6.3 as an option within alternatives command sudo alternatives --install /usr/bin/mvn mvn /usr/local/maven/apache-maven-3.6.3/bin/mvn 1 // Use alternatives to choose mvn version sudo alternatives --config mvn","title":"maven installation"},{"location":"User-Guide/#hadoop-native-librarydefault","text":"Please make sure you have set up Hadoop directory properly with Hadoop Native Libraries By default, Apache Arrow would scan $HADOOP_HOME and find the native Hadoop library libhdfs.so (under $HADOOP_HOME/lib/native directory) to be used for Hadoop client. You can also use ARROW_LIBHDFS_DIR to configure the location of libhdfs.so if it is installed in other directory than $HADOOP_HOME/lib/native If your SPARK and HADOOP are separated in different nodes, please find libhdfs.so in your Hadoop cluster and copy it to SPARK cluster, then use one of the above methods to set it properly. For more information, please check Arrow HDFS interface documentation Hadoop Native Library, please read the official Hadoop website documentation","title":"Hadoop Native Library(Default)"},{"location":"User-Guide/#use-libhdfs3-library-for-better-performanceoptional","text":"For better performance ArrowDataSource reads HDFS files using the third-party library libhdfs3 . The library must be pre-installed on machines Spark Executor nodes are running on. To install the library, use of Conda is recommended. // installing libhdfs3 conda install -c conda-forge libhdfs3 // check the installed library file ll ~/miniconda/envs/$(YOUR_ENV_NAME)/lib/libhdfs3.so To set up libhdfs3, there are two different ways: Option1: Overwrite the soft link for libhdfs.so To install libhdfs3.so, you have to create a soft link for libhdfs.so in your Hadoop directory( $HADOOP_HOME/lib/native by default). ln -f -s libhdfs3.so libhdfs.so Option2: Add env variable to the system export ARROW_LIBHDFS3_DIR=\"PATH_TO_LIBHDFS3_DIR/\" Add following Spark configuration options before running the DataSource to make the library to be recognized: * spark.executorEnv.ARROW_LIBHDFS3_DIR = \"PATH_TO_LIBHDFS3_DIR/\" * spark.executorEnv.LD_LIBRARY_PATH = \"PATH_TO_LIBHDFS3_DEPENDENCIES_DIR/\" Please notes: If you choose to use libhdfs3.so, there are some other dependency libraries you have to installed such as libprotobuf or libcrypto.","title":"Use libhdfs3 library for better performance(Optional)"},{"location":"User-Guide/#build-and-install-intel-optimized-arrow-with-datasets-java-api","text":"You have to use a customized Arrow to support for our datasets Java API. // build arrow-cpp git clone -b <version> https://github.com/Intel-bigdata/arrow.git cd arrow/cpp mkdir build cd build cmake -DARROW_DEPENDENCY_SOURCE=BUNDLED -DARROW_GANDIVA_JAVA=ON -DARROW_GANDIVA=ON -DARROW_PARQUET=ON -DARROW_HDFS=ON -DARROW_BOOST_USE_SHARED=ON -DARROW_JNI=ON -DARROW_DATASET=ON -DARROW_WITH_PROTOBUF=ON -DARROW_WITH_SNAPPY=ON -DARROW_WITH_LZ4=ON -DARROW_FILESYSTEM=ON -DARROW_JSON=ON .. make // build and install arrow jvm library cd ../../java mvn clean install -P arrow-jni -am -Darrow.cpp.build.dir=$PATH_TO_ARROW_SOURCE_CODE/arrow/cpp/build/release","title":"Build and install Intel\u00ae Optimized Arrow with Datasets Java API"},{"location":"User-Guide/#build-arrow-data-source-library","text":"// Download Arrow Data Source Code git clone -b <version> https://github.com/oap-project/arrow-data-source.git // Go to the directory cd arrow-data-source // build mvn clean -DskipTests package // check built jar library readlink -f standard/target/spark-arrow-datasource-standard-<version>-jar-with-dependencies.jar","title":"Build Arrow Data Source Library"},{"location":"User-Guide/#download-spark-300","text":"Currently ArrowDataSource works on the Spark 3.0.0 version. wget http://archive.apache.org/dist/spark/spark-3.0.0/spark-3.0.0-bin-hadoop2.7.tgz tar -xf ./spark-3.0.0-bin-hadoop2.7.tgz export SPARK_HOME=`pwd`/spark-3.0.0-bin-hadoop2.7 If you are new to Apache Spark, please go though Spark's official deploying guide before getting started with ArrowDataSource.","title":"Download Spark 3.0.0"},{"location":"User-Guide/#get-started","text":"","title":"Get started"},{"location":"User-Guide/#add-extra-class-pathes-to-spark","text":"To enable ArrowDataSource, the previous built jar spark-arrow-datasource-standard-<version>-jar-with-dependencies.jar should be added to Spark configuration. Typically the options are: spark.driver.extraClassPath : Set to load jar file to driver. spark.executor.extraClassPath : Set to load jar file to executor. jars : Set to copy jar file to the executors when using yarn cluster mode. spark.executorEnv.ARROW_LIBHDFS3_DIR : Optional if you are using a custom libhdfs3.so. spark.executorEnv.LD_LIBRARY_PATH : Optional if you are using a custom libhdfs3.so. For Spark Standalone Mode, please set the above value as relative path to the jar file. For Spark Yarn Cluster Mode, please set the above value as absolute path to the jar file. Example to run Spark Shell with ArrowDataSource jar file ${SPARK_HOME}/bin/spark-shell \\ --verbose \\ --master yarn \\ --driver-memory 10G \\ --conf spark.driver.extraClassPath=$PATH_TO_DATASOURCE_DIR/spark-arrow-datasource-standard-<version>-jar-with-dependencies.jar \\ --conf spark.executor.extraClassPath=$PATH_TO_DATASOURCE_DIR/spark-arrow-datasource-standard-<version>-jar-with-dependencies.jar \\ --conf spark.driver.cores=1 \\ --conf spark.executor.instances=12 \\ --conf spark.executor.cores=6 \\ --conf spark.executor.memory=20G \\ --conf spark.memory.offHeap.size=80G \\ --conf spark.task.cpus=1 \\ --conf spark.locality.wait=0s \\ --conf spark.sql.shuffle.partitions=72 \\ --conf spark.executorEnv.ARROW_LIBHDFS3_DIR=\"$PATH_TO_LIBHDFS3_DIR/\" \\ --conf spark.executorEnv.LD_LIBRARY_PATH=\"$PATH_TO_LIBHDFS3_DEPENDENCIES_DIR\" For more information about these options, please read the official Spark documentation .","title":"Add extra class pathes to Spark"},{"location":"User-Guide/#run-a-query-with-arrowdatasource-scala","text":"val path = \"${PATH_TO_YOUR_PARQUET_FILE}\" val df = spark.read .option(ArrowOptions.KEY_ORIGINAL_FORMAT, \"parquet\") .option(ArrowOptions.KEY_FILESYSTEM, \"hdfs\") .format(\"arrow\") .load(path) df.createOrReplaceTempView(\"my_temp_view\") spark.sql(\"SELECT * FROM my_temp_view LIMIT 10\").show(10)","title":"Run a query with ArrowDataSource (Scala)"},{"location":"User-Guide/#to-validate-if-arrowdatasource-works-properly","text":"To validate if ArrowDataSource works, you can go to the DAG to check if ArrowScan has been used from the above example query.","title":"To validate if ArrowDataSource works properly"},{"location":"User-Guide/#work-together-with-parquetdatasource-experimental","text":"We provide a customized replacement of Spark's built-in ParquetFileFormat. By so users don't have to change existing Parquet-based SQL/code and will be able to read Arrow data from Parquet directly. More importantly, sometimes the feature could be extremely helpful to make ArrowDataSource work correctly with some 3rd-party storage tools (e.g. Delta Lake ) that are built on top of ParquetDataSource. To replace built-in ParquetDataSource, the only thing has to be done is to place compiled jar spark-arrow-datasource-parquet-<version>.jar into Spark's library folder. If you'd like to verify that ParquetDataSource is successfully overwritten by the jar, run following code before executing SQL job: ServiceLoaderUtil.ensureParquetFileFormatOverwritten(); Note the whole feature is currently experimental and only DataSource v1 is supported. V2 support is being planned.","title":"Work together with ParquetDataSource (experimental)"}]}